{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688740ff-28bb-4285-bdef-fb3070176233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4388, 11)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Firefox_dataset.csv')\n",
    "\n",
    "# Check if 'priority' column exists\n",
    "if 'priority' in df.columns:\n",
    "    # Create an empty DataFrame for the sampled data\n",
    "    sampled_df = pd.DataFrame()\n",
    "\n",
    "    # Get unique priorities\n",
    "    unique_priorities = df['priority'].unique()\n",
    "\n",
    "    for priority in unique_priorities:\n",
    "        # Filter the DataFrame by priority\n",
    "        df_priority = df[df['priority'] == priority]\n",
    "        \n",
    "        # Sample rows for the current priority\n",
    "        if len(df_priority) >= 1000:\n",
    "            sampled_rows = df_priority.sample(n=1000, random_state=1)  # Ensuring reproducibility with random_state\n",
    "        else:\n",
    "            # If less than 1000 rows, take all\n",
    "            sampled_rows = df_priority\n",
    "        \n",
    "        # Append the sampled rows to the sampled_df DataFrame\n",
    "        sampled_df = pd.concat([sampled_df, sampled_rows], ignore_index=True)\n",
    "else:\n",
    "    print(\"The dataset does not contain a 'priority' column.\")\n",
    "name = \"firefox_Pri_sampled_dataset_1k.csv\"  \n",
    "# Save the sampled DataFrame to a new CSV file if needed\n",
    "sampled_df.to_csv('firefox_Pri_sampled_dataset_1k.csv', index=False)\n",
    "\n",
    "# Output the shape of the sampled dataset to confirm\n",
    "sampled_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7debdd8-beae-4736-bf46-f27d66e87fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method IndexOpsMixin.value_counts of 0       5\n",
      "1       5\n",
      "2       5\n",
      "3       5\n",
      "4       5\n",
      "       ..\n",
      "4383    4\n",
      "4384    4\n",
      "4385    4\n",
      "4386    4\n",
      "4387    4\n",
      "Name: priority, Length: 4388, dtype: int64>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(name)\n",
    "\n",
    "# Check if 'priority' column exists\n",
    "if 'priority' in df.columns:\n",
    "    # Define a mapping from old values to new ones\n",
    "    priority_mapping = {\n",
    "        'P1': 1,\n",
    "        'P2': 2,\n",
    "        'P3': 3,\n",
    "        'P4': 4,\n",
    "        'P5':5\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "\n",
    "    # Replace the priority values based on the mapping\n",
    "    df['priority'] = df['priority'].replace(priority_mapping)\n",
    "else:\n",
    "    print(\"The dataset does not contain a 'priority' column.\")\n",
    "\n",
    "df['priority'] = pd.to_numeric(df['priority'], errors='coerce')\n",
    "\n",
    "# Handle NaN values in 'priority' column. You can either drop them or fill them.\n",
    "# Here, we'll drop any rows with NaN in 'priority'. Alternatively, you can fill with a value, e.g., df['priority'].fillna(0, inplace=True)\n",
    "df.dropna(subset=['priority'], inplace=True)\n",
    "print(df['priority'].value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca874f9-4159-4d01-87cb-500226d169db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "      <th>product</th>\n",
       "      <th>component</th>\n",
       "      <th>type</th>\n",
       "      <th>keywords</th>\n",
       "      <th>priority</th>\n",
       "      <th>status</th>\n",
       "      <th>blocks</th>\n",
       "      <th>depends_on</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1276496</td>\n",
       "      <td>Privacy at risk (linux)</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Untriaged</td>\n",
       "      <td>defect</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>blocker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1799656</td>\n",
       "      <td>No way to restore the popups browser message f...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Site Permissions</td>\n",
       "      <td>defect</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>NEW</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1272292</td>\n",
       "      <td>Show default search engine in search field again</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Search</td>\n",
       "      <td>defect</td>\n",
       "      <td>['blocked-ux']</td>\n",
       "      <td>5</td>\n",
       "      <td>NEW</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>S4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1606198</td>\n",
       "      <td>Intermittent browser/base/content/test/tabcras...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Tabbed Browser</td>\n",
       "      <td>defect</td>\n",
       "      <td>['crash', 'intermittent-failure']</td>\n",
       "      <td>5</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1863860</td>\n",
       "      <td>Intermittent browser/components/shopping/tests...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Messaging System</td>\n",
       "      <td>defect</td>\n",
       "      <td>['intermittent-failure', 'intermittent-testcas...</td>\n",
       "      <td>5</td>\n",
       "      <td>NEW</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>S4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>1610256</td>\n",
       "      <td>Sign + Publish v1.0.1 of searchengines-devtool...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Search</td>\n",
       "      <td>task</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1608164]</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>1614393</td>\n",
       "      <td>Sign + Publish v1.0.2 of searchengines-devtool...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Search</td>\n",
       "      <td>task</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>1621031</td>\n",
       "      <td>The “Collection Promo” section is wrongly redi...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>New Tab Page</td>\n",
       "      <td>defect</td>\n",
       "      <td>['regression']</td>\n",
       "      <td>4</td>\n",
       "      <td>RESOLVED</td>\n",
       "      <td>[1566597]</td>\n",
       "      <td>[]</td>\n",
       "      <td>S4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>1623331</td>\n",
       "      <td>Firefox Sync clears most bookmark folders</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Sync</td>\n",
       "      <td>defect</td>\n",
       "      <td>['stalled']</td>\n",
       "      <td>4</td>\n",
       "      <td>UNCONFIRMED</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>1623466</td>\n",
       "      <td>Cannot screenshot some websites [e.g. Tantek.c...</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Screenshots</td>\n",
       "      <td>defect</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>REOPENED</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1789727]</td>\n",
       "      <td>S4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4388 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                            summary  product  \\\n",
       "0     1276496                            Privacy at risk (linux)  Firefox   \n",
       "1     1799656  No way to restore the popups browser message f...  Firefox   \n",
       "2     1272292   Show default search engine in search field again  Firefox   \n",
       "3     1606198  Intermittent browser/base/content/test/tabcras...  Firefox   \n",
       "4     1863860  Intermittent browser/components/shopping/tests...  Firefox   \n",
       "...       ...                                                ...      ...   \n",
       "4383  1610256  Sign + Publish v1.0.1 of searchengines-devtool...  Firefox   \n",
       "4384  1614393  Sign + Publish v1.0.2 of searchengines-devtool...  Firefox   \n",
       "4385  1621031  The “Collection Promo” section is wrongly redi...  Firefox   \n",
       "4386  1623331          Firefox Sync clears most bookmark folders  Firefox   \n",
       "4387  1623466  Cannot screenshot some websites [e.g. Tantek.c...  Firefox   \n",
       "\n",
       "             component    type  \\\n",
       "0            Untriaged  defect   \n",
       "1     Site Permissions  defect   \n",
       "2               Search  defect   \n",
       "3       Tabbed Browser  defect   \n",
       "4     Messaging System  defect   \n",
       "...                ...     ...   \n",
       "4383            Search    task   \n",
       "4384            Search    task   \n",
       "4385      New Tab Page  defect   \n",
       "4386              Sync  defect   \n",
       "4387       Screenshots  defect   \n",
       "\n",
       "                                               keywords  priority  \\\n",
       "0                                                    []         5   \n",
       "1                                                    []         5   \n",
       "2                                        ['blocked-ux']         5   \n",
       "3                     ['crash', 'intermittent-failure']         5   \n",
       "4     ['intermittent-failure', 'intermittent-testcas...         5   \n",
       "...                                                 ...       ...   \n",
       "4383                                                 []         4   \n",
       "4384                                                 []         4   \n",
       "4385                                     ['regression']         4   \n",
       "4386                                        ['stalled']         4   \n",
       "4387                                                 []         4   \n",
       "\n",
       "           status     blocks depends_on severity  \n",
       "0        RESOLVED         []         []  blocker  \n",
       "1             NEW         []         []       S3  \n",
       "2             NEW         []         []       S4  \n",
       "3        RESOLVED         []         []       S3  \n",
       "4             NEW         []         []       S4  \n",
       "...           ...        ...        ...      ...  \n",
       "4383     RESOLVED         []  [1608164]   normal  \n",
       "4384     RESOLVED         []         []   normal  \n",
       "4385     RESOLVED  [1566597]         []       S4  \n",
       "4386  UNCONFIRMED         []         []       S3  \n",
       "4387     REOPENED         []  [1789727]       S4  \n",
       "\n",
       "[4388 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99251e01-0031-45f0-a871-9a1fd3ece6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severity\n",
      "3    2737\n",
      "1    1057\n",
      "2     395\n",
      "4      36\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1544674/3537270922.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['severity'] = df['severity'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Check if 'severity' column exists\n",
    "if 'severity' in df.columns:\n",
    "    # Define a mapping from old values to new ones\n",
    "    priority_mapping = {\n",
    "        'S3': 1,\n",
    "        'S4': 2,\n",
    "        'normal': 3,\n",
    "        'critical':4\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "\n",
    "    # Replace the severity values based on the mapping\n",
    "    df['severity'] = df['severity'].map(priority_mapping)\n",
    "\n",
    "    # Now, we only want to keep rows where 'severity' is not NaN after mapping\n",
    "    # This effectively removes rows with values not in our mapping\n",
    "    df = df.dropna(subset=['severity'])\n",
    "\n",
    "    # Optionally, convert 'severity' to an integer type if all mappings are integers\n",
    "    df['severity'] = df['severity'].astype(int)\n",
    "else:\n",
    "    print(\"The dataset does not contain a 'severity' column.\")\n",
    "\n",
    "# If you want to see the value counts for the 'severity' column\n",
    "print(df['severity'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "420f3300-1732-4121-9dc6-6babde89880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There was a problem when trying to write in your cache folder (/home/jovyan/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n",
      "/tmp/ipykernel_1544674/2659291508.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df.dropna(subset=['severity', 'product', 'component', 'priority', 'status'], inplace=True)\n",
      "/tmp/ipykernel_1544674/2659291508.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df['summary'] = firefox_issues_df['summary'].str.lower()\n",
      "/tmp/ipykernel_1544674/2659291508.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df['summary'] = firefox_issues_df['summary'].apply(lambda x: re.sub(r'[^a-z0-9\\s]', '', x))\n",
      "/tmp/ipykernel_1544674/2659291508.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df.replace('--', pd.NA, inplace=True)\n",
      "/tmp/ipykernel_1544674/2659291508.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df.dropna(subset=['severity', 'priority'], inplace=True)\n",
      "/tmp/ipykernel_1544674/2659291508.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df['component_encoded'] = component_encoder.fit_transform(firefox_issues_df['component'])\n",
      "/tmp/ipykernel_1544674/2659291508.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df['status_encoded'] = status_encoder.fit_transform(firefox_issues_df['status'])\n",
      "/tmp/ipykernel_1544674/2659291508.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df['severity_encoded'] = severity_encoder.fit_transform(firefox_issues_df['severity'])\n",
      "/tmp/ipykernel_1544674/2659291508.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df['keywords_str'] = firefox_issues_df['keywords'].apply(lambda x: ' '.join(ast.literal_eval(x)) if pd.notnull(x) else '')\n",
      "/tmp/ipykernel_1544674/2659291508.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df[[ 'component_encoded', 'status_encoded', 'severity_encoded']] = scaled_features\n",
      "/tmp/ipykernel_1544674/2659291508.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df['blocks_count'] = firefox_issues_df['blocks'].apply(lambda x: len(ast.literal_eval(x)) if pd.notnull(x) else 0)\n",
      "/tmp/ipykernel_1544674/2659291508.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df['depends_on_count'] = firefox_issues_df['depends_on'].apply(lambda x: len(ast.literal_eval(x)) if pd.notnull(x) else 0)\n",
      "/tmp/ipykernel_1544674/2659291508.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  firefox_issues_df[['blocks_count', 'depends_on_count']] = scaled_count_features\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# # # Load the dataset\n",
    "# file_path = \"Core_Pri_sampled_dataset.csv\"\n",
    "# # firefox_issues_df = balanced_df\n",
    "# firefox_issues_df = pd.read_csv(file_path)\n",
    "firefox_issues_df =df \n",
    "\n",
    "# Clean and preprocess data\n",
    "firefox_issues_df.dropna(subset=['severity', 'product', 'component', 'priority', 'status'], inplace=True)\n",
    "firefox_issues_df['summary'] = firefox_issues_df['summary'].str.lower()\n",
    "firefox_issues_df['summary'] = firefox_issues_df['summary'].apply(lambda x: re.sub(r'[^a-z0-9\\s]', '', x))\n",
    "\n",
    "firefox_issues_df.replace('--', pd.NA, inplace=True)\n",
    "firefox_issues_df.dropna(subset=['severity', 'priority'], inplace=True)\n",
    "\n",
    "# Encode 'Product', 'Component', and 'Status' using label encoding\n",
    "product_encoder = LabelEncoder()\n",
    "component_encoder = LabelEncoder()\n",
    "status_encoder = LabelEncoder()\n",
    "# firefox_issues_df['product_encoded'] = product_encoder.fit_transform(firefox_issues_df['product'])\n",
    "firefox_issues_df['component_encoded'] = component_encoder.fit_transform(firefox_issues_df['component'])\n",
    "firefox_issues_df['status_encoded'] = status_encoder.fit_transform(firefox_issues_df['status'])\n",
    "# Encode 'severity' using label encoding\n",
    "severity_encoder = LabelEncoder()\n",
    "firefox_issues_df['severity_encoded'] = severity_encoder.fit_transform(firefox_issues_df['severity'])\n",
    "\n",
    "# Convert 'keywords' list to a string\n",
    "firefox_issues_df['keywords_str'] = firefox_issues_df['keywords'].apply(lambda x: ' '.join(ast.literal_eval(x)) if pd.notnull(x) else '')\n",
    "\n",
    "# Use CountVectorizer to encode 'keywords' (This step could be memory-intensive for large datasets)\n",
    "vectorizer = CountVectorizer()\n",
    "keywords_encoded = vectorizer.fit_transform(firefox_issues_df['keywords_str'])\n",
    "\n",
    "# Initialize BERT tokenizer and model with a specified cache directory\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir='cache')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', cache_dir='cache')\n",
    "\n",
    "\n",
    "# from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# # Initialize RoBERTa tokenizer and model\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base' ,cache_dir='cache')\n",
    "# model = RobertaModel.from_pretrained('roberta-base', cache_dir='cache')\n",
    "\n",
    "\n",
    "# Set the device to GPU (if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Function for batch processing of summaries to obtain BERT embeddings\n",
    "def batch_encode_summaries(summaries, tokenizer, model, batch_size=16):\n",
    "    dataloader = DataLoader(summaries, batch_size=batch_size, shuffle=False)\n",
    "    text_features_list = []\n",
    "\n",
    "    for batch_summaries in dataloader:\n",
    "        inputs = tokenizer(batch_summaries, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        batch_text_features = outputs.last_hidden_state.mean(dim=1)\n",
    "        text_features_list.append(batch_text_features.cpu())  # Move to CPU to avoid GPU memory overload\n",
    "\n",
    "    return torch.cat(text_features_list, dim=0)\n",
    "\n",
    "\n",
    "# Encode summaries in batches to get text features\n",
    "text_features = batch_encode_summaries(firefox_issues_df['summary'].tolist(), tokenizer, model, batch_size=16)\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Include 'severity_encoded' in the features to scale\n",
    "features_to_scale = firefox_issues_df[[ 'component_encoded', 'status_encoded', 'severity_encoded']]\n",
    "\n",
    "# Fit the scaler to the features and transform them to a 0-1 range\n",
    "scaled_features = scaler.fit_transform(features_to_scale)\n",
    "\n",
    "# Update the dataframe with the scaled features\n",
    "# firefox_issues_df[['product_encoded', 'component_encoded', 'status_encoded']] = scaled_features\n",
    "# Corrected line to include 'severity_encoded' in the DataFrame update\n",
    "firefox_issues_df[[ 'component_encoded', 'status_encoded', 'severity_encoded']] = scaled_features\n",
    "\n",
    "\n",
    "# Convert the 'keywords_encoded' sparse matrix to a tensor\n",
    "keywords_tensor = torch.tensor(keywords_encoded.toarray(), dtype=torch.float)\n",
    "\n",
    "# Calculate 'count of blocks' and 'count of depends_on'\n",
    "firefox_issues_df['blocks_count'] = firefox_issues_df['blocks'].apply(lambda x: len(ast.literal_eval(x)) if pd.notnull(x) else 0)\n",
    "firefox_issues_df['depends_on_count'] = firefox_issues_df['depends_on'].apply(lambda x: len(ast.literal_eval(x)) if pd.notnull(x) else 0)\n",
    "\n",
    "# Scale the counts\n",
    "count_features = firefox_issues_df[['blocks_count', 'depends_on_count']]\n",
    "scaled_count_features = scaler.fit_transform(count_features)\n",
    "firefox_issues_df[['blocks_count', 'depends_on_count']] = scaled_count_features\n",
    "\n",
    "# Prepare the features tensor, now including 'severity_encoded' and 'keywords_tensor'\n",
    "features_tensor = torch.tensor(firefox_issues_df[[ 'component_encoded', 'status_encoded', 'severity_encoded', 'blocks_count', 'depends_on_count']].values, dtype=torch.float)\n",
    "\n",
    "# Concatenate the BERT embeddings with the scaled features\n",
    "features = torch.cat((text_features, features_tensor), dim=1)\n",
    "\n",
    "# # Prepare edge index and map issue IDs to node indices\n",
    "# node_id_mapping = {node_id: idx for idx, node_id in enumerate(firefox_issues_df['id'])}\n",
    "# edge_index = []\n",
    "\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity_matrix = cosine_similarity(text_features.numpy())\n",
    "\n",
    "# Define a similarity threshold\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# Prepare edge index\n",
    "edge_index = []\n",
    "node_id_mapping = {node_id: idx for idx, node_id in enumerate(firefox_issues_df['id'])}\n",
    "\n",
    "for i in range(similarity_matrix.shape[0]):\n",
    "    for j in range(i + 1, similarity_matrix.shape[1]):\n",
    "        if similarity_matrix[i, j] >= similarity_threshold:\n",
    "            edge_index.append([i, j])\n",
    "            edge_index.append([j, i])  # Adding reverse edge for undirected graph\n",
    "\n",
    "edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Convert the 'priority' and 'severity' columns to tensors\n",
    "# priority_labels = torch.tensor(firefox_issues_df['priority'].values, dtype=torch.long)\n",
    "severity_labels = torch.tensor(firefox_issues_df['priority'].values, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch Geometric Data object\n",
    "data = Data(x=features, edge_index=edge_index_tensor, y_severity=severity_labels)\n",
    "\n",
    "# Save the Data object\n",
    "data_save_path = 'firefox_issues_data.pt'\n",
    "torch.save(data, data_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a66d11-d0c1-4de5-a0a1-b3b3e4275cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4225\n",
      "Number of edges: 12830\n",
      "Average node degree: 3.04\n",
      "Number of features per node: 773\n"
     ]
    }
   ],
   "source": [
    "# Number of Nodes\n",
    "num_nodes = data.num_nodes\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "\n",
    "# Number of Edges\n",
    "num_edges = data.num_edges\n",
    "print(f\"Number of edges: {num_edges}\")\n",
    "\n",
    "# Average Node Degree\n",
    "avg_degree = num_edges / num_nodes\n",
    "print(f\"Average node degree: {avg_degree:.2f}\")\n",
    "\n",
    "# Number of Features per Node\n",
    "num_features = data.num_features\n",
    "print(f\"Number of features per node: {num_features}\")\n",
    "\n",
    "# # Checking for Isolated Nodes\n",
    "# num_isolated_nodes = sum(data.degree() == 0).item()\n",
    "# print(f\"Number of isolated nodes: {num_isolated_nodes}\")\n",
    "\n",
    "# # Checking for Self-loops\n",
    "# num_self_loops = data.contains_self_loops().item()\n",
    "# print(f\"Number of self-loops: {num_self_loops}\")\n",
    "\n",
    "# # Graph Density\n",
    "# density = num_edges / (num_nodes * (num_nodes - 1))\n",
    "# print(f\"Graph density: {density:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfd5e364-f2e7-469f-91a1-48555521b2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severity labels within expected range [0, 5].\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have defined num_classes_priority and num_classes_severity\n",
    "# num_classes_priority = 6  # Example: 6 priority classes\n",
    "num_classes_severity = 6  # Example: 6 severity classes\n",
    "\n",
    "# Check the range for priority labels\n",
    "# priority_label_min = data.y_priority.min().item()\n",
    "# priority_label_max = data.y_priority.max().item()\n",
    "\n",
    "# if priority_label_min < 0 or priority_label_max >= num_classes_priority:\n",
    "#     print(f\"Priority labels out of expected range [0, {num_classes_priority-1}]: Min = {priority_label_min}, Max = {priority_label_max}\")\n",
    "# else:\n",
    "    # print(f\"Priority labels within expected range [0, {num_classes_priority-1}].\")\n",
    "\n",
    "# Check the range for severity labels\n",
    "severity_label_min = data.y_severity.min().item()\n",
    "severity_label_max = data.y_severity.max().item()\n",
    "\n",
    "if severity_label_min < 0 or severity_label_max >= num_classes_severity:\n",
    "    print(f\"Severity labels out of expected range [0, {num_classes_severity-1}]: Min = {severity_label_min}, Max = {severity_label_max}\")\n",
    "else:\n",
    "    print(f\"Severity labels within expected range [0, {num_classes_severity-1}].\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b42b8eaa-5e98-4fab-b204-15de39b99507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "\n",
    "class HybridGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes_severity, num_units):\n",
    "        super(HybridGNN, self).__init__()\n",
    "\n",
    "        # First layer is a GraphSAGE layer\n",
    "        self.conv1 = SAGEConv(num_node_features, num_units)\n",
    "\n",
    "        # Second layer is a GAT layer with multi-head attention\n",
    "        self.conv2 = GATConv(num_units, num_units // 2, heads=2, concat=True)\n",
    "\n",
    "        # Final output features adjusted for concatenated multi-head attention\n",
    "        final_out_features = num_units  # Assuming concat=True doubles the feature size\n",
    "\n",
    "        # Define separate layers for 'priority' and 'severity' with adjusted class counts\n",
    "        # self.out_priority = torch.nn.Linear(final_out_features, 7)  # Adjusted for 'priority'\n",
    "        self.out_severity = torch.nn.Linear(final_out_features, 6)  # Correct for 'severity'\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # GraphSAGE convolution\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)  # Apply dropout after GraphSAGE\n",
    "\n",
    "        # GAT convolution\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)  # ELU activation for GAT\n",
    "        x = F.dropout(x, training=self.training)  # Apply dropout after GAT\n",
    "\n",
    "        # Output layers for 'priority' and 'severity'\n",
    "        # priority = self.out_priority(x)\n",
    "        severity = self.out_severity(x)\n",
    "\n",
    "        return F.log_softmax(severity, dim=1)\n",
    "\n",
    "# Instantiate the model with the same number of units\n",
    "# model = HybridGNN(num_node_features=770, num_classes_priority=7, num_classes_severity=6, num_units=110)\n",
    "model = HybridGNN(num_node_features=773, num_classes_severity=6, num_units=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0eb67ce7-c00f-4f36-b47b-9b2e674eb45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severity labels within expected range [0, 6].\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have defined num_classes_priority and num_classes_severity\n",
    "num_classes_priority = 6  # Example: 6 priority classes\n",
    "num_classes_severity = 7  # Example: 6 severity classes\n",
    "\n",
    "\n",
    "# Check the range for severity labels\n",
    "severity_label_min = data.y_severity.min().item()\n",
    "severity_label_max = data.y_severity.max().item()\n",
    "\n",
    "if severity_label_min < 0 or severity_label_max >= num_classes_severity:\n",
    "    print(f\"Severity labels out of expected range [0, {num_classes_severity-1}]: Min = {severity_label_min}, Max = {severity_label_max}\")\n",
    "else:\n",
    "    print(f\"Severity labels within expected range [0, {num_classes_severity-1}].\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ded33001-2f70-4fc8-aa50-d5ec70bb85cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severity labels within expected range [0, 6].\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have defined num_classes_priority and num_classes_severity\n",
    "num_classes_priority = 4  # Example: 6 priority classes\n",
    "num_classes_severity = 7  # Example: 6 severity classes\n",
    "\n",
    "\n",
    "# Check the range for severity labels\n",
    "severity_label_min = data.y_severity.min().item()\n",
    "severity_label_max = data.y_severity.max().item()\n",
    "\n",
    "if severity_label_min < 0 or severity_label_max >= num_classes_severity:\n",
    "    print(f\"Severity labels out of expected range [0, {num_classes_severity-1}]: Min = {severity_label_min}, Max = {severity_label_max}\")\n",
    "else:\n",
    "    print(f\"Severity labels within expected range [0, {num_classes_severity-1}].\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b767f0a5-23f8-4aaa-8920-f3598d65bc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HybridGNN(\n",
       "  (conv1): SAGEConv(773, 128, aggr=mean)\n",
       "  (conv2): GATConv(128, 64, heads=2)\n",
       "  (out_severity): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'Core_large_sum_model.pth'\n",
    "\n",
    "model = HybridGNN(num_node_features=773, num_classes_severity=6, num_units=128)  # Adjust parameters as needed\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "324c9f95-fdff-4ed1-b49e-3da79016ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adjusting the output layer for a different number of classes\n",
    "# This is necessary only if the new dataset has a different number of classes than the original\n",
    "num_classes_new_dataset = 6  # Adjust this based on your new dataset\n",
    "model.out_severity = torch.nn.Linear(model.out_severity.in_features, num_classes_new_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a14bdda-7824-4f2f-b686-72aed8865268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the training set: 2957\n",
      "Number of nodes in the validation set: 633\n",
      "Number of nodes in the test set: 635\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'data' is your PyTorch Geometric Data object\n",
    "\n",
    "# Calculate the total number of nodes\n",
    "num_nodes = data.x.size(0)\n",
    "\n",
    "# Define the split sizes\n",
    "train_size = 0.70  # 70% of the data for training\n",
    "val_size = 0.15  # 15% of the data for validation\n",
    "test_size = 0.15  # 15% of the data for testing\n",
    "\n",
    "# Generate shuffled indices\n",
    "indices = torch.randperm(num_nodes)\n",
    "\n",
    "# Calculate the number of nodes for each split\n",
    "num_train_nodes = int(train_size * num_nodes)\n",
    "num_val_nodes = int(val_size * num_nodes)\n",
    "\n",
    "# Split the indices for each set\n",
    "train_indices = indices[:num_train_nodes]\n",
    "val_indices = indices[num_train_nodes:num_train_nodes + num_val_nodes]\n",
    "test_indices = indices[num_train_nodes + num_val_nodes:]\n",
    "\n",
    "# Create boolean masks\n",
    "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "data.train_mask[train_indices] = True\n",
    "data.val_mask[val_indices] = True\n",
    "data.test_mask[test_indices] = True\n",
    "\n",
    "# Calculate the number of nodes in each set\n",
    "num_nodes_train = data.train_mask.sum().item()\n",
    "num_nodes_val = data.val_mask.sum().item()\n",
    "num_nodes_test = data.test_mask.sum().item()\n",
    "\n",
    "print(f\"Number of nodes in the training set: {num_nodes_train}\")\n",
    "print(f\"Number of nodes in the validation set: {num_nodes_val}\")\n",
    "print(f\"Number of nodes in the test set: {num_nodes_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0140ce67-fb8e-487a-b775-e6adc19264d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the training set: 2957\n",
      "Number of nodes in the validation set: 633\n",
      "Number of nodes in the test set: 635\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'data' is your PyTorch Geometric Data object\n",
    "\n",
    "# Calculate the total number of nodes\n",
    "num_nodes = data.x.size(0)\n",
    "\n",
    "# Define the split sizes\n",
    "train_size = 0.70  # 70% of the data for training\n",
    "val_size = 0.15  # 15% of the data for validation\n",
    "test_size = 0.15  # 15% of the data for testing\n",
    "\n",
    "# Generate shuffled indices\n",
    "indices = torch.randperm(num_nodes)\n",
    "\n",
    "# Calculate the number of nodes for each split\n",
    "num_train_nodes = int(train_size * num_nodes)\n",
    "num_val_nodes = int(val_size * num_nodes)\n",
    "\n",
    "# Split the indices for each set\n",
    "train_indices = indices[:num_train_nodes]\n",
    "val_indices = indices[num_train_nodes:num_train_nodes + num_val_nodes]\n",
    "test_indices = indices[num_train_nodes + num_val_nodes:]\n",
    "\n",
    "# Create boolean masks\n",
    "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "data.train_mask[train_indices] = True\n",
    "data.val_mask[val_indices] = True\n",
    "data.test_mask[test_indices] = True\n",
    "\n",
    "# Calculate the number of nodes in each set\n",
    "num_nodes_train = data.train_mask.sum().item()\n",
    "num_nodes_val = data.val_mask.sum().item()\n",
    "num_nodes_test = data.test_mask.sum().item()\n",
    "\n",
    "print(f\"Number of nodes in the training set: {num_nodes_train}\")\n",
    "print(f\"Number of nodes in the validation set: {num_nodes_val}\")\n",
    "print(f\"Number of nodes in the test set: {num_nodes_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3fafdf1-c56d-4630-96d2-66e3fbf762c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_priority = torch.nn.CrossEntropyLoss()\n",
    "criterion_severity = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17399627-869d-4d75-b2b9-eda67e66ddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/300, Total Loss: 1.8657, Severity Loss: 1.8657\n",
      "Epoch: 10/300, Total Loss: 1.8696, Severity Loss: 1.8696\n",
      "Epoch: 20/300, Total Loss: 1.8798, Severity Loss: 1.8798\n",
      "Epoch: 30/300, Total Loss: 1.8723, Severity Loss: 1.8723\n",
      "Epoch: 40/300, Total Loss: 1.8643, Severity Loss: 1.8643\n",
      "Epoch: 50/300, Total Loss: 1.8516, Severity Loss: 1.8516\n",
      "Epoch: 60/300, Total Loss: 1.8668, Severity Loss: 1.8668\n",
      "Epoch: 70/300, Total Loss: 1.8688, Severity Loss: 1.8688\n",
      "Epoch: 80/300, Total Loss: 1.8686, Severity Loss: 1.8686\n",
      "Epoch: 90/300, Total Loss: 1.8702, Severity Loss: 1.8702\n",
      "Epoch: 100/300, Total Loss: 1.8667, Severity Loss: 1.8667\n",
      "Epoch: 110/300, Total Loss: 1.8680, Severity Loss: 1.8680\n",
      "Epoch: 120/300, Total Loss: 1.8649, Severity Loss: 1.8649\n",
      "Epoch: 130/300, Total Loss: 1.8712, Severity Loss: 1.8712\n",
      "Epoch: 140/300, Total Loss: 1.8678, Severity Loss: 1.8678\n",
      "Epoch: 150/300, Total Loss: 1.8675, Severity Loss: 1.8675\n",
      "Epoch: 160/300, Total Loss: 1.8644, Severity Loss: 1.8644\n",
      "Epoch: 170/300, Total Loss: 1.8515, Severity Loss: 1.8515\n",
      "Epoch: 180/300, Total Loss: 1.8668, Severity Loss: 1.8668\n",
      "Epoch: 190/300, Total Loss: 1.8664, Severity Loss: 1.8664\n",
      "Epoch: 200/300, Total Loss: 1.8563, Severity Loss: 1.8563\n",
      "Epoch: 210/300, Total Loss: 1.8701, Severity Loss: 1.8701\n",
      "Epoch: 220/300, Total Loss: 1.8674, Severity Loss: 1.8674\n",
      "Epoch: 230/300, Total Loss: 1.8497, Severity Loss: 1.8497\n",
      "Epoch: 240/300, Total Loss: 1.8786, Severity Loss: 1.8786\n",
      "Epoch: 250/300, Total Loss: 1.8535, Severity Loss: 1.8535\n",
      "Epoch: 260/300, Total Loss: 1.8606, Severity Loss: 1.8606\n",
      "Epoch: 270/300, Total Loss: 1.8763, Severity Loss: 1.8763\n",
      "Epoch: 280/300, Total Loss: 1.8645, Severity Loss: 1.8645\n",
      "Epoch: 290/300, Total Loss: 1.8713, Severity Loss: 1.8713\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300  # Define the number of epochs\n",
    "log_interval = 10  # Interval at which to log training status\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    severity_logits = model(data.x, data.edge_index)\n",
    "\n",
    "    # Apply the mask to the entire node-wise prediction tensor\n",
    "    # masked_priority_logits = priority_logits[data.train_mask]\n",
    "    masked_severity_logits = severity_logits[data.train_mask]\n",
    "\n",
    "    # Get the corresponding labels for the masked nodes\n",
    "    # masked_y_priority = data.y_priority[data.train_mask.nonzero(as_tuple=True)[0]]\n",
    "    masked_y_severity = data.y_severity[data.train_mask.nonzero(as_tuple=True)[0]]\n",
    "\n",
    "    # Compute loss for each task\n",
    "    # loss_priority = criterion_priority(masked_priority_logits, masked_y_priority)\n",
    "    loss_severity = criterion_severity(masked_severity_logits, masked_y_severity)\n",
    "\n",
    "    # Combine losses and perform backpropagation\n",
    "    loss = loss_severity  # You might want to weight these losses differently\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log training information\n",
    "    if epoch % log_interval == 0:\n",
    "        print(f'Epoch: {epoch}/{num_epochs}, Total Loss: {loss.item():.4f}, '\n",
    "              f'Severity Loss: {loss_severity.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91d24f2c-5d0d-4cde-a897-df55f3a89abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss_severity loss 1.8150712251663208\n",
      "Test Loss: 1.8150712251663208\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Forward pass using the entire graph\n",
    "    severity_logits = model(data.x, data.edge_index)\n",
    "\n",
    "    # Apply the test mask to logits and labels for loss calculation\n",
    "\n",
    "    test_severity_logits = severity_logits[data.test_mask]\n",
    "\n",
    "    # Get the corresponding labels for the test mask\n",
    "    test_y_severity = data.y_severity[data.test_mask.nonzero(as_tuple=True)[0]]\n",
    "\n",
    "    # Compute loss for each task\n",
    "    test_loss_severity = criterion_severity(test_severity_logits, test_y_severity)\n",
    "    test_loss =  test_loss_severity\n",
    "\n",
    "\n",
    "    print(\"test_loss_severity loss\", test_loss_severity.item())\n",
    "    print(f'Test Loss: {test_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "695e0150-93bf-490f-be05-b453e78d190f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Severities: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4]\n",
      "Predicted Severities: [0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 0 5 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 3 1 1 5 0 0 1 1 1 5 0 0 3 3 3 0 5 1 1 0 1 1 0 0 3 3 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 3 5 5 0 1 1 0 3 1 0 0 5 5 0 0 3 1 1 1 0 1 0 0 0 0 0 0 3\n",
      " 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 3 5 1 5 1 1 1 5 3 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 3 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 3 0 0 3 3 1 3 0 0 5 5 0\n",
      " 1 0 0 0 0 1 0 0 5 0 0 3 0 1 0 1 5 5 3 3 0 3 3 0 0 1 0 1 1 1 1 1 0 5 1 3 1\n",
      " 1 1 0 3 5 0 5 3 0 0 5 0 3 0 3 3 0 3 5 5 0 0 0 1 0 3 3 0 3 3 3 3 5 3 3 5 3\n",
      " 0 0 1 5 1 0 1 5 0 0 0 0 0 5 1 1 5 5 0 0 1 1 0 0 0 0 0 3 1 1 0 0 5 1 5 3 0\n",
      " 3 1 0 5 0 3 0 0 1 1 0 3 1 5 1 0 0 0 5 3 0 0 0 0 3 3 3 5 0 1 0 0 0 3 1 3 5\n",
      " 5 3 0 0 0 0 0 5 0 3 3 5 3 0 1 3 0 0 0 3 0 0 0 0 0 0 0 3 3 0 0 3 0 1 3 3 0\n",
      " 5 1 3 0 0 0 0 0 0 0 0 3 1 1 0 3 3 5 0 0 1 0 0 1 3 1 1 5 1 0 3 0 0 3 1 0 1\n",
      " 3 3 0 1 5 3 0 5 5 5 1 1 0 1 3 3 1 3 0 5 1 0 0 5 0 0 1 0 5 3 1 3 0 0 3 3 0\n",
      " 3 3 0 0 1 1 3 3 0 1 0 0 3 0 3 0 0 3 3 0 0 3 5 3 0 0 3 0 3 0 5 5 3 3 0 0 0\n",
      " 3 3 0 3 0 3 0 0 0 3 0 1 1 0 5 3 0 1 0 0 0 0 0 0 0 3 0 0 0 0 3 0 0 0 0 0 0\n",
      " 3 0 0 0 0 0]\n",
      "Severity - Accuracy: 0.0756, Precision: 0.0719, Recall: 0.0756, F1: 0.0710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jagrit.acharya1/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jagrit.acharya1/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Forward pass using the entire graph\n",
    "    severity_logits = model(data.x, data.edge_index)\n",
    "\n",
    "    test_severity_logits = severity_logits[data.test_mask]\n",
    "\n",
    "    test_y_severity = data.y_severity[data.test_mask.nonzero(as_tuple=True)[0]]\n",
    "\n",
    "\n",
    "    _, predicted_severities = torch.max(test_severity_logits, 1)\n",
    "\n",
    "    true_severities = test_y_severity.cpu().numpy()\n",
    "\n",
    "    predicted_severities = predicted_severities.cpu().numpy()\n",
    "\n",
    "\n",
    "    print(\"Actual Severities:\", true_severities)\n",
    "    print(\"Predicted Severities:\", predicted_severities)\n",
    "\n",
    "    # Calculate metrics for 'severity'\n",
    "    accuracy_severity = accuracy_score(true_severities, predicted_severities)\n",
    "    precision_severity = precision_score(true_severities, predicted_severities, average='weighted')\n",
    "    recall_severity = recall_score(true_severities, predicted_severities, average='weighted')\n",
    "    f1_severity = f1_score(true_severities, predicted_severities, average='weighted')\n",
    "\n",
    "\n",
    "    print(f\"Severity - Accuracy: {accuracy_severity:.4f}, Precision: {precision_severity:.4f}, \"\n",
    "          f\"Recall: {recall_severity:.4f}, F1: {f1_severity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "217d5bb8-a199-465e-bc0b-415b901ca467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1976, -1.2144, -1.7368, -1.4379, -1.7230],\n",
      "        [-1.8490, -1.4435, -1.5796, -1.2818, -2.0964],\n",
      "        [-1.6140, -1.6849, -1.4749, -1.6585, -1.6286],\n",
      "        ...,\n",
      "        [-2.9674, -1.4644, -1.6285, -1.3799, -1.3112],\n",
      "        [-1.6933, -1.5448, -1.4252, -1.4727, -2.0176],\n",
      "        [-1.7995, -2.1198, -1.5370, -1.1387, -1.7188]],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(masked_output)  \u001b[38;5;66;03m# Check what the masked output looks like\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Then try indexing with labels\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mnew_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Assuming new_data.y exists and is correctly set up\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)  \u001b[38;5;66;03m# Check labels\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da400aaf-d33b-47f4-9bfb-7fbc3d4605d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m test_logits \u001b[38;5;241m=\u001b[39m logits[new_data\u001b[38;5;241m.\u001b[39mtest_mask]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Assuming 'new_data.y' is your labels tensor for the new dataset\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m \u001b[43mnew_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_mask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Get the predictions\u001b[39;00m\n\u001b[1;32m     19\u001b[0m _, predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(test_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
